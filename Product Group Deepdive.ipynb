{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Group Deepdive\n",
    "Previously in the **Interproduct Tweet Analysis** we didn't find any substantial conversion between product groups. In this analysis we will attempt to create monthly topic models for tweets from each product groups. The goal is to study how topics differ between product groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import itertools as it\n",
    "import os\n",
    "import pickle\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "CURRENT_DIR = os.path.abspath(os.curdir)\n",
    "DATA_PICKLE_DIR = os.path.join(CURRENT_DIR, 'Data\\\\Pickles\\\\')\n",
    "ASSETS_DIR = os.path.join(CURRENT_DIR, 'Assets\\\\')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data through wordclouds\n",
    "First we will create a word cloud (of onegrams, and bigrams) for each month's tweets for each product group. This will give us a good indication of what the major topics will be. We can then use these as starter points to find more topics, and to classify tweets into different topics.\n",
    "\n",
    "In the **Interproduct Tweet Analysis** we found that almost 80% of the monthly tweets' users tweet only once (on one of the product groups). This indicates a presence of two groups of users in our dataset (infrequent users, and frequent users). Because of the huge number of users who tweeted only once, we will consider them seperately from the remaining tweets.\n",
    "\n",
    "First we need to load our grouped tweets. We will do this product wise; starting with *Vape*.\n",
    "\n",
    "### Vape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserActivityStat(enum.Enum):\n",
    "    InFrequent = 1\n",
    "    Medium = 2\n",
    "    Frequent = 3\n",
    "\n",
    "def load_data(product_group):\n",
    "    filename = os.path.join(DATA_PICKLE_DIR, f'ProductGroupedDFs-{product_group}.pickle')\n",
    "    with open(filename, 'rb') as file_handle:\n",
    "        data = pickle.load(file_handle)\n",
    "        print('File loaded.')\n",
    "    # Classify tweet as FromInfrequentUser or not.\n",
    "    for datum in data:\n",
    "        user_counts = datum.df.groupby('UserId').size()\n",
    "        def _get_user_activity_stat(tweet):\n",
    "            if user_counts.loc[tweet.UserId] > 1:\n",
    "                return UserActivityStat.Frequent\n",
    "            else:\n",
    "                return UserActivityStat.InFrequent\n",
    "        datum.df['UserActivityStat'] = datum.df.apply(_get_user_activity_stat, axis=1)\n",
    "        print(f'Processed {datum.date_label}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n",
      "Processed Apr 2017\n",
      "Processed May 2017\n",
      "Processed Jun 2017\n",
      "Processed Jul 2017\n",
      "Processed Aug 2017\n",
      "Processed Sep 2017\n",
      "Processed Oct 2017\n",
      "Processed Nov 2017\n",
      "Processed Dec 2017\n",
      "Processed Jan 2018\n",
      "Processed Feb 2018\n",
      "Processed Mar 2018\n"
     ]
    }
   ],
   "source": [
    "vape = load_data('vape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find the counts for the onegrams and bigrams in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ngrams(data):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    for datum in data:\n",
    "        onegrams, bigrams = {}, {}\n",
    "        for row in datum.df.itertuples():\n",
    "            prev = None\n",
    "            for word in (x for x in row.NormalizedText if x not in stopwords):\n",
    "                if word not in onegrams:\n",
    "                    onegrams[word] = SimpleNamespace(frequent=0, infrequent=0, total=0)\n",
    "                if row.UserActivityStat == UserActivityStat.Frequent:\n",
    "                    onegrams[word].frequent += 1\n",
    "                elif row.UserActivityStat == UserActivityStat.InFrequent:\n",
    "                    onegrams[word].infrequent += 1\n",
    "                onegrams[word].total += 1\n",
    "\n",
    "                if prev is not None:\n",
    "                    bigram = f'{prev}-{word}'\n",
    "                    if bigram not in bigrams:\n",
    "                        bigrams[bigram] = SimpleNamespace(frequent=0, infrequent=0, total=0)\n",
    "                    if row.UserActivityStat == UserActivityStat.Frequent:\n",
    "                        bigrams[bigram].frequent += 1\n",
    "                    elif row.UserActivityStat == UserActivityStat.InFrequent:\n",
    "                        bigrams[bigram].infrequent += 1\n",
    "                    bigrams[bigram].total += 1\n",
    "                prev = word\n",
    "        datum.onegrams = onegrams\n",
    "        datum.bigrams = bigrams\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vape = process_ngrams(vape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use the ngrams to create the wordclouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordclouds(data, product_name, w=800, h=600):\n",
    "    onegram_save_folder = os.path.join(ASSETS_DIR, f'WordClouds\\\\{product_name}\\\\Onegrams')\n",
    "    if not os.path.exists(onegram_save_folder):\n",
    "        os.makedirs(onegram_save_folder)\n",
    "        os.makedirs(os.path.join(onegram_save_folder, 'Frequent'))\n",
    "        os.makedirs(os.path.join(onegram_save_folder, 'InFrequent'))\n",
    "        os.makedirs(os.path.join(onegram_save_folder, 'Total'))\n",
    "    bigram_save_folder = os.path.join(ASSETS_DIR, f'WordClouds\\\\{product_name}\\\\Bigrams')\n",
    "    if not os.path.exists(bigram_save_folder):\n",
    "        os.makedirs(bigram_save_folder)\n",
    "        os.makedirs(os.path.join(bigram_save_folder, 'Frequent'))\n",
    "        os.makedirs(os.path.join(bigram_save_folder, 'InFrequent'))\n",
    "        os.makedirs(os.path.join(bigram_save_folder, 'Total'))\n",
    "    for datum in data:\n",
    "        wc = WordCloud(width=w, height=h).generate_from_frequencies({key:val.frequent for key, val in datum.onegrams.items()})\n",
    "        wc.to_file(os.path.join(onegram_save_folder, f'Frequent\\\\{datum.date_label}.png'))\n",
    "        wc = WordCloud(width=w, height=h).generate_from_frequencies({key:val.infrequent for key, val in datum.onegrams.items()})\n",
    "        wc.to_file(os.path.join(onegram_save_folder, f'InFrequent\\\\{datum.date_label}.png'))\n",
    "        wc = WordCloud(width=w, height=h).generate_from_frequencies({key:val.total for key, val in datum.onegrams.items()})\n",
    "        wc.to_file(os.path.join(onegram_save_folder, f'Total\\\\{datum.date_label}.png'))\n",
    "\n",
    "        wc = WordCloud(width=w, height=h).generate_from_frequencies({key:val.frequent for key, val in datum.bigrams.items()})\n",
    "        wc.to_file(os.path.join(bigram_save_folder, f'Frequent\\\\{datum.date_label}.png'))\n",
    "        wc = WordCloud(width=w, height=h).generate_from_frequencies({key:val.infrequent for key, val in datum.bigrams.items()})\n",
    "        wc.to_file(os.path.join(bigram_save_folder, f'InFrequent\\\\{datum.date_label}.png'))\n",
    "        wc = WordCloud(width=w, height=h).generate_from_frequencies({key:val.total for key, val in datum.bigrams.items()})\n",
    "        wc.to_file(os.path.join(bigram_save_folder, f'Total\\\\{datum.date_label}.png'))\n",
    "        print(f'Generated wordclouds for {datum.date_label}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordclouds(vape, 'vape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cigarette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n",
      "Processed Apr 2017\n",
      "Processed May 2017\n",
      "Processed Jun 2017\n",
      "Processed Jul 2017\n",
      "Processed Aug 2017\n",
      "Processed Sep 2017\n",
      "Processed Oct 2017\n",
      "Processed Nov 2017\n",
      "Processed Dec 2017\n",
      "Processed Jan 2018\n",
      "Processed Feb 2018\n",
      "Processed Mar 2018\n",
      "Generated wordclouds for Apr 2017.\n",
      "Generated wordclouds for May 2017.\n",
      "Generated wordclouds for Jun 2017.\n",
      "Generated wordclouds for Jul 2017.\n",
      "Generated wordclouds for Aug 2017.\n",
      "Generated wordclouds for Sep 2017.\n",
      "Generated wordclouds for Oct 2017.\n",
      "Generated wordclouds for Nov 2017.\n",
      "Generated wordclouds for Dec 2017.\n",
      "Generated wordclouds for Jan 2018.\n",
      "Generated wordclouds for Feb 2018.\n",
      "Generated wordclouds for Mar 2018.\n"
     ]
    }
   ],
   "source": [
    "cigarette = load_data('cigarette')\n",
    "process_ngrams(cigarette)\n",
    "create_wordclouds(cigarette, 'cigarette')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hookah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n",
      "Processed Apr 2017\n",
      "Processed May 2017\n",
      "Processed Jun 2017\n",
      "Processed Jul 2017\n",
      "Processed Aug 2017\n",
      "Processed Sep 2017\n",
      "Processed Oct 2017\n",
      "Processed Nov 2017\n",
      "Processed Dec 2017\n",
      "Processed Jan 2018\n",
      "Processed Feb 2018\n",
      "Processed Mar 2018\n"
     ]
    }
   ],
   "source": [
    "hookah = load_data('hookah')\n",
    "# process_ngrams(hookah)\n",
    "# create_wordclouds(hookah, 'hookah')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n",
      "Processed Apr 2017\n",
      "Processed May 2017\n",
      "Processed Jun 2017\n",
      "Processed Jul 2017\n",
      "Processed Aug 2017\n",
      "Processed Sep 2017\n",
      "Processed Oct 2017\n",
      "Processed Nov 2017\n",
      "Processed Dec 2017\n",
      "Processed Jan 2018\n",
      "Processed Feb 2018\n",
      "Processed Mar 2018\n"
     ]
    }
   ],
   "source": [
    "swisher = load_data('swisher')\n",
    "# process_ngrams(swisher)\n",
    "# create_wordclouds(swisher, 'swisher')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([datum.df.UserId for datum in hookah])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154245,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
